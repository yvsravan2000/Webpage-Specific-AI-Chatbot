{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description:  NLP Project - To develop a Program interface that provides a 'self learning' chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title:  Webpage Specific AI ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules and libraries required: \n",
    "# ['Docx', 'Google Translator', 'NLTK', 'Newspaper', 'NumPy', 'Pyttsx3', 'Random', 'Sklearn', 'Speech Recognition', 'String', 'Winsound']\n",
    "# Packages used:\n",
    "# ['Punkt', 'Wordnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import io\n",
    "import sys\n",
    "import nltk\n",
    "import random\n",
    "import string \n",
    "import pyttsx3\n",
    "import warnings\n",
    "import winsound\n",
    "import contextlib\n",
    "import numpy as np\n",
    "from docx import Document\n",
    "from heapq import nlargest\n",
    "from newspaper import Article\n",
    "from docx.shared import Inches\n",
    "from string import punctuation\n",
    "import speech_recognition as sr\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading packages\n",
    "nltk.download('punkt', quiet=True)                                           # Download the punkt package\n",
    "nltk.download('wordnet', quiet=True)                                         # Download the wordnet package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input: Webpage URL\n",
    "inp_url = str(input(\"Enter the webpage/article URL: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text extraction from webpage\n",
    "article = Article(inp_url)\n",
    "article.download()                                                           # Download the article\n",
    "article.parse()                                                              # Parse the article\n",
    "article.nlp()                                 \n",
    "# Apply Natural Language Processing (NLP)\n",
    "corpus = article.text                                                        # Store the article text into corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text extracted\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "text = corpus\n",
    "sent_tokens = nltk.sent_tokenize(text)                                       # Text to a list of sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary (Key: Value) to remove punctuations  \n",
    "remove_punct_dict = dict(  (ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to return a list of lemmatized lower case words after removing punctuations \n",
    "def LemNormalize(text):\n",
    "    return nltk.word_tokenize(text.lower().translate(remove_punct_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization of webpage text content\n",
    "class FrequencySummarizer:\n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        self.min_cut = min_cut\n",
    "        self.max_cut = max_cut\n",
    "        self.stopwords = set(stopwords.words(\"english\")+ list(punctuation))  \n",
    "        \n",
    "    def _compute_frequencies(self, word_sent):                               # Compute the frequency of each word\n",
    "        freq = defaultdict(int)\n",
    "        for s in word_sent:\n",
    "            for word in s:\n",
    "                if word not in self.stopwords:\n",
    "                    freq[word] +=1                                           # Frequencies normalization and filtering\n",
    "        m = float(max(freq.values()))\n",
    "        for w in freq.keys():\n",
    "            freq[w] = freq[w]/m\n",
    "            if freq[w] >= self.max_cut or freq[w] <= self.min_cut:\n",
    "                del freq[w]\n",
    "            return freq\n",
    "    \n",
    "    def summarize(self, text, n):\n",
    "            sents = sent_tokenize(text)\n",
    "            assert n <= len(sents)\n",
    "            word_sent = [word_tokenize(s.lower()) for s in sents]\n",
    "            self._freq = self._compute_frequencies(word_sent)\n",
    "            ranking = defaultdict(int)\n",
    "            for i,sent in enumerate(word_sent):\n",
    "                for w in sent:\n",
    "                    if w in self._freq:\n",
    "                        ranking[i] += self._freq[w]\n",
    "            sents_idx = self._rank(ranking,n)\n",
    "            return [sents[j] for j in sents_idx]\n",
    "        \n",
    "    def _rank(self, ranking, n):\n",
    "            return nlargest(n, ranking, key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greetings and responses initialization\n",
    "# Greeting input from the user\n",
    "GREETING_INPUTS = [\"hi\", \"hello\",  \"hola\", \"greetings\", \"hey\", \"hai\"] \n",
    "# Greeting responses back to the user\n",
    "GREETING_RESPONSES = [\"hello\",\"hi\", \"hey\", \"what's good\",  \"great to see you here\",\"hey there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Injecting speech characterisitics to the WeBot\n",
    "frequency = 2500                                                             # Set Frequency To 2500 Hertz\n",
    "duration = 250                                                               # Set Duration To 1000 ms == 1 second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating voice engine for WeBot\n",
    "engine = pyttsx3.init()\n",
    "engine.setProperty('volume',0.8)\n",
    "engine.setProperty('rate', 150)\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a random greeting response to a users greeting\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating responses\n",
    "def response(user_response):\n",
    "    robo_response='' # Create an empty response for the bot\n",
    "    sent_tokens.append(user_response) # Append the users response to the list of sentence tokens\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    score = flat[-2]\n",
    "    if(score==0):\n",
    "        robo_response = robo_response + \"I apologize, I didn't understand.\"\n",
    "    else:\n",
    "        robo_response = robo_response + sent_tokens[idx]\n",
    "    sent_tokens.remove(user_response) \n",
    "       \n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Print Output\n",
    "@contextlib.contextmanager\n",
    "def suppress_output():\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = io.StringIO()\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating chat environment for real-time interactions\n",
    "flag=True\n",
    "r = sr.Recognizer()\n",
    "chat_text = ''\n",
    "winsound.Beep(frequency, duration)\n",
    "intro_msg = \"I am Website Specific AI Chatbot or WeBot for short. I will answer your queries about this website. If you want to exit, say Exit!\"\n",
    "chat_text = chat_text + \"Bot:\" + intro_msg + \"\\n\"\n",
    "print(\"WeBot: \", intro_msg)\n",
    "engine.say(intro_msg)\n",
    "engine.runAndWait()\n",
    "while(flag==True):    \n",
    "    print(\"-------------------------------------------------- Speak --------------------------------------------------\")\n",
    "    winsound.Beep(frequency, duration)\n",
    "    with sr.Microphone() as source:\n",
    "        audio_text = r.listen(source)\n",
    "    winsound.Beep(frequency, duration)\n",
    "    try:\n",
    "        with suppress_output():\n",
    "            user_response = r.recognize_google(audio_text) \n",
    "    except:\n",
    "        user_response = \"--- voice not recognized ---\"\n",
    "    \n",
    "    user_response=user_response.lower()\n",
    "    print(\"You: \", user_response)\n",
    "    chat_text = chat_text + \"You: \"+ user_response + \"\\n\"\n",
    "    if(user_response!='exit'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"WeBot: Always at your service!\")\n",
    "            chat_text = chat_text + \"Bot: \"+ \"Always at your service!\" + \"\\n\"\n",
    "            engine.say(\"Always at your service!\")\n",
    "            engine.runAndWait()\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                g_ur = greeting(user_response)\n",
    "                print(\"WeBot: \"+g_ur)\n",
    "                chat_text = chat_text + \"Bot: \"+ g_ur + \"\\n\"\n",
    "                engine.say(g_ur)\n",
    "                engine.runAndWait()\n",
    "            else:\n",
    "                r_ur = response(user_response)\n",
    "                print(\"WeBot: \"+r_ur)\n",
    "                chat_text = chat_text + \"Bot: \"+ r_ur + \"\\n\"\n",
    "                engine.say(r_ur)\n",
    "                engine.runAndWait()\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"WeBot: Chat with you later!\")\n",
    "        chat_text = chat_text + \"Bot: \"+ \"Chat with you later!\" + \"\\n\"\n",
    "        engine.say(\"Chat with you later\")\n",
    "        engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the summarized text\n",
    "text = text.replace(\"\\n\", \"\")\n",
    "line_count = 0\n",
    "summarized_content = \"\\n\\nWebpage Content Summary:\"\n",
    "fs = FrequencySummarizer();\n",
    "for ranked_sentence in fs.summarize(text,2):\n",
    "    line_count += 1\n",
    "    summarized_content = summarized_content + \"\\n\\n --> \" + ranked_sentence   \n",
    "chat_text = chat_text + summarized_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving chat to the users local document with added feature of the Indian languages along with world languages\n",
    "document = Document()\n",
    "translator = Translator()\n",
    "languages = {'en':'English', 'hi': 'Hindi', 'te':'Telugu', 'ta':'Tamil', 'kn':'Kannada', 'mr':'Marathi', 'ml':'Malayalam', 'fr':'French', 'de':'German'}\n",
    "document.add_heading('WeBot Chat - Translation', 0)\n",
    "for language in languages:\n",
    "    translated_text = translator.translate(chat_text, dest=language).text\n",
    "    document.add_heading(languages[language], level=1)\n",
    "    document.add_paragraph(translated_text)\n",
    "    \n",
    "document.add_page_break()\n",
    "document.save('WeBot-Output.docx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
